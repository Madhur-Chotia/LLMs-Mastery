{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u3IdkvB37DC"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = load_dataset(\"hakurei/open-instruct-v1\", split='train')\n",
        "dataset.to_pandas().sample(20)\n"
      ],
      "metadata": {
        "id": "daPCUFbL5Rzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(example):\n",
        "    example['prompt'] = f\"{example['instruction']} {example['input']} {example['output']}\"\n",
        "\n",
        "    return example\n",
        "\n",
        "def tokenize_datasets(dataset):\n",
        "    tokenized_dataset = dataset.map(lambda example: tokenizer(example['prompt'], truncation=True, max_length=128), batched=True, remove_columns=['prompt'])\n",
        "\n",
        "    return tokenized_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "zBZIsMlP5S6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess, remove_columns=['instruction', 'input', 'output'])\n",
        "dataset =  dataset.shuffle(42).select(range(100000)).train_test_split(test_size=0.1, seed=42)"
      ],
      "metadata": {
        "id": "bkcGbuyJ5UlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "6v3CMwkY5Xxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "train_dataset = tokenize_datasets(train_dataset)\n",
        "test_dataset = tokenize_datasets(test_dataset)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "hmcHU4u55bP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "traing_args = TrainingArguments(output_dir=\"models/diablo_gpt\",\n",
        "                                num_train_epochs=1,\n",
        "                                per_device_train_batch_size=32,\n",
        "                                per_device_eval_batch_size=32)\\\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                    args=traing_args,\n",
        "                    train_dataset=train_dataset,\n",
        "                    eval_dataset=test_dataset,\n",
        "                    data_collator=data_collator)\n"
      ],
      "metadata": {
        "id": "8cg_KDw05c5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "wId35B9u5eSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the trained checkpoint directly\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TheFuzzyScientist/diabloGPT_open-instruct\")"
      ],
      "metadata": {
        "id": "SkwVr9KMOnJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_text(prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    outputs = model.generate(inputs, max_length=64, pad_token_id=tokenizer.eos_token_id)\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated[:generated.rfind('.')+1]"
      ],
      "metadata": {
        "id": "cWA3k57Y5gUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_text(\"What's the best way to cook chiken breast?\")\n",
        "\n",
        "generate_text(\"Should I invest stocks?\")\n",
        "\n",
        "generate_text(\"I need a place to go for this summer vacation, what locations would you recommend\")\n",
        "\n",
        "generate_text(\"What's the fastest route from NY City to Boston?\")\n",
        "\n"
      ],
      "metadata": {
        "id": "e5aItzuB5hj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}